// gpu_path_tracer.cpp — GPU wavefront path tracer implementation.
//
// Manages the full wavefront path tracing pipeline on the GPU:
//   Generate → for each bounce: Extend → Shade → Connect
//   Final: readback accumulated color to CPU buffer.
//
// Shares the local RenderingDevice from GPURayCaster.  Owns all
// wavefront-specific GPU buffers (rays, intersections, path state,
// shadow rays, materials, lights, accumulation).

#include "gpu_path_tracer.h"
#include "api/ray_service.h"
#include "core/ray.h"
#include "core/intersection.h"

#include <godot_cpp/classes/rendering_device.hpp>
#include <godot_cpp/classes/rd_shader_spirv.hpp>
#include <godot_cpp/classes/rd_shader_source.hpp>
#include <godot_cpp/classes/rd_uniform.hpp>
#include <godot_cpp/classes/rd_sampler_state.hpp>
#include <godot_cpp/classes/rd_pipeline_specialization_constant.hpp>
#include <godot_cpp/variant/packed_byte_array.hpp>
#include <godot_cpp/variant/utility_functions.hpp>

#include <cstring>

using namespace godot;

// ============================================================================
// Embedded shader sources (generated by SConstruct from .comp.glsl files)
// ============================================================================

#include "gpu/shaders/pt_generate.gen.h"
#include "gpu/shaders/pt_shade.gen.h"
#include "gpu/shaders/cwbvh_traverse.gen.h"

// ============================================================================
// Constants
// ============================================================================

/// Workgroup size for Generate and Shade kernels (1D dispatch).
/// 128 threads matches CWBVH traversal and gives good GPU occupancy.
static constexpr uint32_t WORKGROUP_SIZE = 128;

/// Maximum GPU dispatch batch size (avoid TDR on long-running shaders).
/// 512K pixels per dispatch, matching GPURayCaster's MAX_GPU_BATCH_SIZE.
static constexpr uint32_t MAX_GPU_BATCH = 524288;

// ============================================================================
// Lifecycle
// ============================================================================

GPUPathTracer::~GPUPathTracer() {
	cleanup();
}

bool GPUPathTracer::initialize(IRayService *svc) {
	RT_ASSERT_NOT_NULL(svc);
	RT_ASSERT(!initialized_ || rd_ != nullptr, "initialize: inconsistent state");

	if (initialized_) { return true; }

	// Get the shared local RenderingDevice from GPURayCaster.
	rd_ = svc->get_gpu_device();
	if (!rd_) {
		WARN_PRINT_ONCE("GPUPathTracer: No GPU device available — cannot initialize.");
		return false;
	}

	// Get scene buffer RIDs (may not be valid yet if build() hasn't been called).
	scene_rids_ = svc->get_gpu_scene_buffer_rids();

	// ---- Compile shaders ----
	generate_shader_ = _compile_shader(PT_GENERATE_GLSL, "pt_generate");
	if (!generate_shader_.is_valid()) {
		ERR_PRINT("GPUPathTracer: Failed to compile Generate shader.");
		cleanup();
		return false;
	}

	shade_shader_ = _compile_shader(PT_SHADE_GLSL, "pt_shade");
	if (!shade_shader_.is_valid()) {
		ERR_PRINT("GPUPathTracer: Failed to compile Shade shader.");
		cleanup();
		return false;
	}

	// Extend and Connect use the same CWBVH traversal shader source
	// but different specialization constants (RAY_MODE).
	extend_shader_ = _compile_shader(CWBVH_TRAVERSE_GLSL, "cwbvh_extend");
	if (!extend_shader_.is_valid()) {
		ERR_PRINT("GPUPathTracer: Failed to compile Extend (CWBVH) shader.");
		cleanup();
		return false;
	}

	connect_shader_ = _compile_shader(CWBVH_TRAVERSE_GLSL, "cwbvh_connect");
	if (!connect_shader_.is_valid()) {
		ERR_PRINT("GPUPathTracer: Failed to compile Connect (CWBVH) shader.");
		cleanup();
		return false;
	}

	// ---- Create compute pipelines ----

	// Generate pipeline — no specialization constants.
	generate_pipeline_ = rd_->compute_pipeline_create(generate_shader_);
	if (!generate_pipeline_.is_valid()) {
		ERR_PRINT("GPUPathTracer: Failed to create Generate pipeline.");
		cleanup();
		return false;
	}

	// Shade pipeline — no specialization constants.
	shade_pipeline_ = rd_->compute_pipeline_create(shade_shader_);
	if (!shade_pipeline_.is_valid()) {
		ERR_PRINT("GPUPathTracer: Failed to create Shade pipeline.");
		cleanup();
		return false;
	}

	// Extend pipeline — RAY_MODE=0 (nearest-hit).
	{
		TypedArray<RDPipelineSpecializationConstant> specs;
		Ref<RDPipelineSpecializationConstant> ray_mode;
		ray_mode.instantiate();
		ray_mode->set_constant_id(0);
		ray_mode->set_value(0);  // nearest-hit
		specs.push_back(ray_mode);
		extend_pipeline_ = rd_->compute_pipeline_create(extend_shader_, specs);
		if (!extend_pipeline_.is_valid()) {
			ERR_PRINT("GPUPathTracer: Failed to create Extend pipeline.");
			cleanup();
			return false;
		}
	}

	// Connect pipeline — RAY_MODE=1 (any-hit, early exit).
	{
		TypedArray<RDPipelineSpecializationConstant> specs;
		Ref<RDPipelineSpecializationConstant> ray_mode;
		ray_mode.instantiate();
		ray_mode->set_constant_id(0);
		ray_mode->set_value(1);  // any-hit
		specs.push_back(ray_mode);
		connect_pipeline_ = rd_->compute_pipeline_create(connect_shader_, specs);
		if (!connect_pipeline_.is_valid()) {
			ERR_PRINT("GPUPathTracer: Failed to create Connect pipeline.");
			cleanup();
			return false;
		}
	}

	// Create texture sampler (linear filter, repeat wrap).
	{
		Ref<RDSamplerState> sampler_state;
		sampler_state.instantiate();
		sampler_state->set_mag_filter(RenderingDevice::SAMPLER_FILTER_LINEAR);
		sampler_state->set_min_filter(RenderingDevice::SAMPLER_FILTER_LINEAR);
		sampler_state->set_repeat_u(RenderingDevice::SAMPLER_REPEAT_MODE_REPEAT);
		sampler_state->set_repeat_v(RenderingDevice::SAMPLER_REPEAT_MODE_REPEAT);
		tex_sampler_ = rd_->sampler_create(sampler_state);
	}

	// Pre-allocate push constant cache.
	push_data_cache_.resize(sizeof(GPUPathTracePush));

	initialized_ = true;
	return true;
}

void GPUPathTracer::cleanup() {
	RT_ASSERT(!initialized_ || rd_ != nullptr, "cleanup: initialized but no RD");
	RT_ASSERT(buffer_pixel_capacity_ == 0 || ray_buffer_.is_valid(),
		"cleanup: capacity > 0 but ray_buffer invalid");

	if (!rd_) { return; }

	_free_wavefront_buffers();
	_free_scene_buffers();
	_free_shaders();

	// Free sampler.
	if (tex_sampler_.is_valid()) {
		rd_->free_rid(tex_sampler_);
		tex_sampler_ = RID();
	}

	initialized_ = false;
	rd_ = nullptr;  // NOT owned — don't destroy.
}

// ============================================================================
// IPathTracer — trace_frame
// ============================================================================

void GPUPathTracer::trace_frame(const PathTraceParams &params,
		Ray *primary_rays,
		float *color_output,
		IRayService *svc,
		IThreadDispatch *pool) {

	RT_ASSERT_NOT_NULL(primary_rays);
	RT_ASSERT_NOT_NULL(color_output);
	RT_ASSERT_NOT_NULL(svc);
	RT_ASSERT_NOT_NULL(pool);
	RT_ASSERT(params.width > 0 && params.height > 0, "Resolution must be positive");

	const uint32_t w = static_cast<uint32_t>(params.width);
	const uint32_t h = static_cast<uint32_t>(params.height);
	const uint32_t pixel_count = w * h;
	if (pixel_count == 0) { return; }

	// Lazy initialization.
	if (!initialized_) {
		if (!initialize(svc)) {
			// Fallback: zero output (black frame).
			std::memset(color_output, 0, pixel_count * 4 * sizeof(float));
			return;
		}
	}

	// Refresh scene buffer RIDs (may have changed after build()).
	scene_rids_ = svc->get_gpu_scene_buffer_rids();
	if (!scene_rids_.cwbvh_valid) {
		WARN_PRINT_ONCE("GPUPathTracer: CWBVH not uploaded — cannot trace.");
		std::memset(color_output, 0, pixel_count * 4 * sizeof(float));
		return;
	}

	// Ensure GPU buffers are sized for this resolution.
	_ensure_wavefront_buffers(pixel_count);

	// Upload scene data if changed (materials, UVs, normals, tangents).
	_upload_shade_data(params.shade, params.lights);

	// Upload per-frame data (camera, environment, lights).
	_upload_frame_data(params);

	// Cache frame parameters for push constant packing in dispatch methods.
	cached_width_  = w;
	cached_height_ = h;
	cached_max_bounces_    = static_cast<uint32_t>(params.max_bounces);
	cached_sample_index_   = params.sample_index;
	cached_light_count_    = static_cast<uint32_t>(params.lights.light_count);
	cached_shadows_enabled_ = params.shadows_enabled;

	// Rebuild descriptor sets if needed.
	_rebuild_descriptor_sets();

	// ---- Wavefront kernel loop ----

	// 1. Generate primary rays + init path state.
	_dispatch_generate(pixel_count);
	rd_->barrier(RenderingDevice::BARRIER_MASK_COMPUTE);

	// 2. Bounce loop: Extend → Shade → Connect
	const uint32_t max_bounces = cached_max_bounces_;
	for (uint32_t bounce = 0; bounce <= max_bounces; bounce++) {
		// Extend: trace active rays through CWBVH (nearest-hit).
		_dispatch_extend(pixel_count);
		rd_->barrier(RenderingDevice::BARRIER_MASK_COMPUTE);

		// Shade: evaluate hits, apply previous NEE, generate shadow + bounce rays.
		_dispatch_shade(pixel_count, bounce);
		rd_->barrier(RenderingDevice::BARRIER_MASK_COMPUTE);

		// Connect: trace shadow rays (any-hit) for stochastic NEE.
		if (params.shadows_enabled && params.lights.light_count > 0) {
			_dispatch_connect(pixel_count);
			rd_->barrier(RenderingDevice::BARRIER_MASK_COMPUTE);
		}
	}

	// 3. Finalize: one last Shade dispatch to apply the final pending NEE
	//    and write tone-mapped RGBA to accum_buffer.
	//    bounce = max_bounces + 1 triggers finalize mode in the shader.
	_dispatch_shade(pixel_count, max_bounces + 1);
	rd_->barrier(RenderingDevice::BARRIER_MASK_COMPUTE);

	// 4. Readback accumulated color to CPU.
	_readback_accumulation(color_output, pixel_count);
}

// ============================================================================
// Shader compilation
// ============================================================================

RID GPUPathTracer::_compile_shader(const char *source, const char *name) {
	RT_ASSERT_NOT_NULL(source);
	RT_ASSERT_NOT_NULL(name);
	RT_ASSERT(rd_ != nullptr, "_compile_shader: RenderingDevice must be valid");

	Ref<RDShaderSource> shader_src;
	shader_src.instantiate();
	shader_src->set_stage_source(RenderingDevice::SHADER_STAGE_COMPUTE, String(source));

	Ref<RDShaderSPIRV> spirv = rd_->shader_compile_spirv_from_source(shader_src);
	if (spirv.is_null()) {
		ERR_PRINT(vformat("GPUPathTracer: SPIRV compilation returned null for '%s'.", name));
		return RID();
	}

	String compile_error = spirv->get_stage_compile_error(RenderingDevice::SHADER_STAGE_COMPUTE);
	if (!compile_error.is_empty()) {
		ERR_PRINT(vformat("GPUPathTracer: Shader '%s' compile error: %s", name, compile_error));
		return RID();
	}

	RID shader = rd_->shader_create_from_spirv(spirv);
	if (!shader.is_valid()) {
		ERR_PRINT(vformat("GPUPathTracer: shader_create_from_spirv failed for '%s'.", name));
	}
	return shader;
}

// ============================================================================
// Buffer management
// ============================================================================

void GPUPathTracer::_ensure_wavefront_buffers(uint32_t pixel_count) {
	RT_ASSERT(pixel_count > 0, "pixel_count must be positive");
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");

	if (pixel_count <= buffer_pixel_capacity_) { return; }

	// Free existing buffers before reallocating.
	_free_wavefront_buffers();

	// Grow with 1.5× factor to reduce reallocation frequency.
	uint32_t new_capacity = static_cast<uint32_t>(pixel_count * 1.5f);
	if (new_capacity < pixel_count) { new_capacity = pixel_count; } // overflow guard

	// Stochastic single-light NEE: one shadow ray per pixel (not per light).
	// This dramatically reduces shadow buffer memory and simplifies the
	// Connect dispatch.  Multi-light NEE is unbiased in expectation through
	// uniform random light selection (weight = N_lights / pdf).

	// Create storage buffers on the GPU.
	ray_buffer_ = rd_->storage_buffer_create(
		new_capacity * sizeof(GPURayPacked));
	intersection_buffer_ = rd_->storage_buffer_create(
		new_capacity * sizeof(GPUIntersectionPacked));
	path_state_buffer_ = rd_->storage_buffer_create(
		new_capacity * sizeof(GPUPathStatePacked));
	shadow_ray_buffer_ = rd_->storage_buffer_create(
		new_capacity * sizeof(GPURayPacked));
	shadow_result_buffer_ = rd_->storage_buffer_create(
		new_capacity * sizeof(uint32_t));
	accum_buffer_ = rd_->storage_buffer_create(
		new_capacity * 4 * sizeof(float));

	buffer_pixel_capacity_ = new_capacity;
}

void GPUPathTracer::_upload_shade_data(const SceneShadeData &shade,
		const SceneLightData &lights) {
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");

	const uint32_t tri_count = static_cast<uint32_t>(shade.triangle_count);
	const uint32_t mat_count = static_cast<uint32_t>(shade.material_count);

	// Skip if scene data hasn't changed.
	if (tri_count == uploaded_triangle_count_ && mat_count == uploaded_material_count_) {
		return;
	}

	// Free old buffers.
	_free_scene_buffers();

	if (mat_count == 0 || tri_count == 0) {
		uploaded_material_count_ = 0;
		uploaded_triangle_count_ = 0;
		return;
	}

	// ---- Pack materials into GPU format ----
	std::vector<GPUMaterialPacked> gpu_mats(mat_count);
	for (uint32_t i = 0; i < mat_count; i++) {
		const MaterialData &m = shade.materials[i];
		GPUMaterialPacked &g = gpu_mats[i];
		g.albedo[0] = m.albedo.r; g.albedo[1] = m.albedo.g; g.albedo[2] = m.albedo.b;
		g.metallic = m.metallic;
		g.emission[0] = m.emission.r; g.emission[1] = m.emission.g; g.emission[2] = m.emission.b;
		g.roughness = m.roughness;
		g.specular = m.specular;
		g.emission_energy = m.emission_energy;
		g.normal_scale = m.normal_scale;
		g.tex_flags = (m.has_albedo_texture ? 1u : 0u) | (m.has_normal_texture ? 2u : 0u);
		g.albedo_tex_idx = -1;  // TODO: texture array index
		g.normal_tex_idx = -1;  // TODO: texture array index
		g.tex_width = m.tex_width;
		g.tex_height = m.tex_height;
	}

	// Upload material buffer.
	{
		uint32_t byte_size = mat_count * sizeof(GPUMaterialPacked);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), gpu_mats.data(), byte_size);
		material_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
	}

	// Upload material ID buffer (per-triangle material index).
	{
		uint32_t byte_size = tri_count * sizeof(uint32_t);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), shade.material_ids, byte_size);
		material_id_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
	}

	// Upload triangle UV data.
	if (shade.triangle_uvs) {
		uint32_t byte_size = tri_count * sizeof(TriangleUV);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), shade.triangle_uvs, byte_size);
		triangle_uv_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
	}

	// Upload vertex normals for smooth shading.
	if (shade.triangle_normals) {
		uint32_t byte_size = tri_count * sizeof(TriangleNormals);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), shade.triangle_normals, byte_size);
		triangle_normal_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
	}

	// Upload tangent data for normal mapping.
	if (shade.triangle_tangents) {
		uint32_t byte_size = tri_count * sizeof(TriangleTangents);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), shade.triangle_tangents, byte_size);
		triangle_tangent_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
	}

	// TODO: Build Texture2DArray for albedo and normal textures.
	// For now, textures are skipped — base material colors are used.

	uploaded_material_count_ = mat_count;
	uploaded_triangle_count_ = tri_count;
}

void GPUPathTracer::_upload_frame_data(const PathTraceParams &params) {
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");
	RT_ASSERT(params.width > 0 && params.height > 0, "Resolution must be positive");

	// ---- Pack environment data ----
	GPUEnvironmentPacked gpu_env;
	const auto &env = params.env;
	gpu_env.sky_zenith[0] = env.sky_zenith_r;
	gpu_env.sky_zenith[1] = env.sky_zenith_g;
	gpu_env.sky_zenith[2] = env.sky_zenith_b;
	gpu_env.ambient_energy = env.ambient_energy;
	gpu_env.sky_horizon[0] = env.sky_horizon_r;
	gpu_env.sky_horizon[1] = env.sky_horizon_g;
	gpu_env.sky_horizon[2] = env.sky_horizon_b;
	gpu_env.ambient_r = env.ambient_r;
	gpu_env.sky_ground[0] = env.sky_ground_r;
	gpu_env.sky_ground[1] = env.sky_ground_g;
	gpu_env.sky_ground[2] = env.sky_ground_b;
	gpu_env.ambient_g = env.ambient_g;
	gpu_env.ambient_b = env.ambient_b;
	gpu_env.tonemap_mode = env.tonemap_mode;
	gpu_env.has_panorama = 0;  // TODO: panorama texture support
	gpu_env._pad = 0;

	// Upload or update environment buffer.
	{
		uint32_t byte_size = sizeof(GPUEnvironmentPacked);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), &gpu_env, byte_size);
		if (env_buffer_.is_valid()) {
			rd_->buffer_update(env_buffer_, 0, byte_size, upload_cache_);
		} else {
			env_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
		}
	}

	// ---- Pack light data ----
	{
		GPULightPacked gpu_lights[MAX_SCENE_LIGHTS];
		std::memset(gpu_lights, 0, sizeof(gpu_lights));
		const int light_count = params.lights.light_count;
		for (int i = 0; i < light_count && i < MAX_SCENE_LIGHTS; i++) {
			const LightData &ld = params.lights.lights[i];
			GPULightPacked &gl = gpu_lights[i];
			gl.position[0] = ld.position.x;
			gl.position[1] = ld.position.y;
			gl.position[2] = ld.position.z;
			gl.range = ld.range;
			gl.direction[0] = ld.direction.x;
			gl.direction[1] = ld.direction.y;
			gl.direction[2] = ld.direction.z;
			gl.attenuation = ld.attenuation;
			gl.color[0] = ld.color.x;
			gl.color[1] = ld.color.y;
			gl.color[2] = ld.color.z;
			gl.spot_angle = ld.spot_angle;
			gl.type = static_cast<uint32_t>(ld.type);
			gl.spot_angle_attenuation = ld.spot_angle_attenuation;
			gl.cast_shadows = ld.cast_shadows ? 1u : 0u;
			gl._pad = 0;
		}

		uint32_t byte_size = MAX_SCENE_LIGHTS * sizeof(GPULightPacked);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), gpu_lights, byte_size);
		if (light_buffer_.is_valid()) {
			rd_->buffer_update(light_buffer_, 0, byte_size, upload_cache_);
		} else {
			light_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
		}
	}

	// ---- Camera data (for GPU ray generation) ----
	{
		const CameraParams &cam = params.camera;
		GPUCameraPacked gpu_cam;
		gpu_cam.origin[0]  = cam.origin[0];
		gpu_cam.origin[1]  = cam.origin[1];
		gpu_cam.origin[2]  = cam.origin[2];
		gpu_cam.fov_y_rad  = cam.fov_y_rad;
		gpu_cam.forward[0] = cam.forward[0];
		gpu_cam.forward[1] = cam.forward[1];
		gpu_cam.forward[2] = cam.forward[2];
		gpu_cam.aspect     = cam.aspect;
		gpu_cam.right[0]   = cam.right[0];
		gpu_cam.right[1]   = cam.right[1];
		gpu_cam.right[2]   = cam.right[2];
		gpu_cam.near_plane = cam.near_plane;
		gpu_cam.up[0]      = cam.up[0];
		gpu_cam.up[1]      = cam.up[1];
		gpu_cam.up[2]      = cam.up[2];
		gpu_cam.far_plane  = cam.far_plane;

		uint32_t byte_size = sizeof(GPUCameraPacked);
		upload_cache_.resize(byte_size);
		std::memcpy(upload_cache_.ptrw(), &gpu_cam, byte_size);
		if (camera_buffer_.is_valid()) {
			rd_->buffer_update(camera_buffer_, 0, byte_size, upload_cache_);
		} else {
			camera_buffer_ = rd_->storage_buffer_create(byte_size, upload_cache_);
		}
	}
}

// ============================================================================
// Descriptor sets
// ============================================================================

void GPUPathTracer::_rebuild_descriptor_sets() {
	RT_ASSERT(rd_ != nullptr, "_rebuild_descriptor_sets: RD must be valid");

	// Helper: create a storage buffer uniform at the given binding.
	auto make_uniform = [](int binding, const RID &buffer) -> Ref<RDUniform> {
		Ref<RDUniform> u;
		u.instantiate();
		u->set_uniform_type(RenderingDevice::UNIFORM_TYPE_STORAGE_BUFFER);
		u->set_binding(binding);
		u->add_id(buffer);
		return u;
	};

	// Free stale descriptor sets before rebuilding.
	auto free_rid = [this](RID &rid) {
		if (rid.is_valid()) { rd_->free_rid(rid); rid = RID(); }
	};

	// ---- Generate descriptor set (3 bindings) ----
	// Bindings: 0=ray_buffer, 1=path_state_buffer, 2=camera_buffer
	if (ray_buffer_.is_valid() && path_state_buffer_.is_valid() &&
			camera_buffer_.is_valid()) {
		free_rid(generate_uniform_set_);
		TypedArray<RDUniform> uniforms;
		uniforms.push_back(make_uniform(0, ray_buffer_));
		uniforms.push_back(make_uniform(1, path_state_buffer_));
		uniforms.push_back(make_uniform(2, camera_buffer_));
		generate_uniform_set_ = rd_->uniform_set_create(uniforms, generate_shader_, 0);
		RT_ASSERT(generate_uniform_set_.is_valid(),
			"_rebuild_descriptor_sets: Generate uniform set creation failed");
	}

	// ---- Extend descriptor set (5 bindings) ----
	// Matches cwbvh_traverse.comp.glsl:
	//   0=cwbvh_nodes, 1=cwbvh_tris, 2=scene_tris, 3=ray_buffer, 4=intersection_buffer
	if (scene_rids_.cwbvh_valid && scene_rids_.scene_valid &&
			ray_buffer_.is_valid() && intersection_buffer_.is_valid()) {
		free_rid(extend_uniform_set_);
		TypedArray<RDUniform> uniforms;
		uniforms.push_back(make_uniform(0, scene_rids_.cwbvh_nodes));
		uniforms.push_back(make_uniform(1, scene_rids_.cwbvh_tris));
		uniforms.push_back(make_uniform(2, scene_rids_.scene_tris));
		uniforms.push_back(make_uniform(3, ray_buffer_));
		uniforms.push_back(make_uniform(4, intersection_buffer_));
		extend_uniform_set_ = rd_->uniform_set_create(uniforms, extend_shader_, 0);
		RT_ASSERT(extend_uniform_set_.is_valid(),
			"_rebuild_descriptor_sets: Extend uniform set creation failed");
	}

	// ---- Shade descriptor set (14 bindings) ----
	// Bindings match pt_shade.comp.glsl layout.
	// Some buffers may not exist yet (e.g., triangle tangent data for scenes
	// without normal maps).  We create zero-sized dummy buffers as placeholders
	// to satisfy the descriptor set layout.
	{
		// Ensure placeholder buffers exist for optional scene data.
		// A 4-byte dummy buffer is sufficient — the shader checks array bounds
		// via push constant counts before accessing.
		auto ensure_buffer = [this](RID &buf, uint32_t min_size = 4) {
			if (!buf.is_valid()) {
				buf = rd_->storage_buffer_create(min_size);
			}
		};
		ensure_buffer(material_buffer_);
		ensure_buffer(material_id_buffer_);
		ensure_buffer(light_buffer_);
		ensure_buffer(env_buffer_);
		ensure_buffer(triangle_uv_buffer_);
		ensure_buffer(triangle_normal_buffer_);
		ensure_buffer(triangle_tangent_buffer_);

		if (ray_buffer_.is_valid() && intersection_buffer_.is_valid() &&
				path_state_buffer_.is_valid() && shadow_ray_buffer_.is_valid() &&
				shadow_result_buffer_.is_valid() && accum_buffer_.is_valid() &&
				scene_rids_.scene_valid) {
			free_rid(shade_uniform_set_);
			TypedArray<RDUniform> uniforms;
			uniforms.push_back(make_uniform(0, ray_buffer_));
			uniforms.push_back(make_uniform(1, intersection_buffer_));
			uniforms.push_back(make_uniform(2, path_state_buffer_));
			uniforms.push_back(make_uniform(3, shadow_ray_buffer_));
			uniforms.push_back(make_uniform(4, shadow_result_buffer_));
			uniforms.push_back(make_uniform(5, accum_buffer_));
			uniforms.push_back(make_uniform(6, material_buffer_));
			uniforms.push_back(make_uniform(7, material_id_buffer_));
			uniforms.push_back(make_uniform(8, light_buffer_));
			uniforms.push_back(make_uniform(9, env_buffer_));
			uniforms.push_back(make_uniform(10, triangle_uv_buffer_));
			uniforms.push_back(make_uniform(11, triangle_normal_buffer_));
			uniforms.push_back(make_uniform(12, triangle_tangent_buffer_));
			uniforms.push_back(make_uniform(13, scene_rids_.scene_tris));
			shade_uniform_set_ = rd_->uniform_set_create(uniforms, shade_shader_, 0);
			RT_ASSERT(shade_uniform_set_.is_valid(),
				"_rebuild_descriptor_sets: Shade uniform set creation failed");
		}
	}

	// ---- Connect descriptor set (5 bindings) ----
	// Same CWBVH shader but with shadow_ray/result buffers instead of primary.
	//   0=cwbvh_nodes, 1=cwbvh_tris, 2=scene_tris, 3=shadow_ray_buffer, 4=shadow_result_buffer
	if (scene_rids_.cwbvh_valid && scene_rids_.scene_valid &&
			shadow_ray_buffer_.is_valid() && shadow_result_buffer_.is_valid()) {
		free_rid(connect_uniform_set_);
		TypedArray<RDUniform> uniforms;
		uniforms.push_back(make_uniform(0, scene_rids_.cwbvh_nodes));
		uniforms.push_back(make_uniform(1, scene_rids_.cwbvh_tris));
		uniforms.push_back(make_uniform(2, scene_rids_.scene_tris));
		uniforms.push_back(make_uniform(3, shadow_ray_buffer_));
		uniforms.push_back(make_uniform(4, shadow_result_buffer_));
		connect_uniform_set_ = rd_->uniform_set_create(uniforms, connect_shader_, 0);
		RT_ASSERT(connect_uniform_set_.is_valid(),
			"_rebuild_descriptor_sets: Connect uniform set creation failed");
	}
}

// ============================================================================
// Kernel dispatch stubs
// ============================================================================

void GPUPathTracer::_dispatch_generate(uint32_t pixel_count) {
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");
	RT_ASSERT(pixel_count > 0, "pixel_count must be positive");
	RT_ASSERT(generate_uniform_set_.is_valid(), "Generate uniform set must be built");

	// Fill push constants with current frame params.
	GPUPathTracePush push{};
	push.pixel_count = pixel_count;
	push.width       = cached_width_;
	push.height      = cached_height_;
	push.bounce      = 0;
	push.max_bounces = cached_max_bounces_;
	push.sample_index = cached_sample_index_;
	push.light_count  = cached_light_count_;
	push.shadows_enabled = cached_shadows_enabled_ ? 1u : 0u;

	std::memcpy(push_data_cache_.ptrw(), &push, sizeof(GPUPathTracePush));

	uint32_t groups = (pixel_count + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE;

	int64_t cl = rd_->compute_list_begin();
	rd_->compute_list_bind_compute_pipeline(cl, generate_pipeline_);
	rd_->compute_list_bind_uniform_set(cl, generate_uniform_set_, 0);
	rd_->compute_list_set_push_constant(cl, push_data_cache_, sizeof(GPUPathTracePush));
	rd_->compute_list_dispatch(cl, groups, 1, 1);
	rd_->compute_list_end();
	rd_->submit();
}

void GPUPathTracer::_dispatch_extend(uint32_t pixel_count) {
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");
	RT_ASSERT(pixel_count > 0, "pixel_count must be positive");
	RT_ASSERT(extend_uniform_set_.is_valid(), "Extend uniform set must be built");

	// CWBVH traversal push constants: ray_count + query_mask.
	GPUPushConstants push{};
	push.ray_count  = pixel_count;
	push.query_mask = 0xFFFFFFFF;  // All layers enabled.

	PackedByteArray push_bytes;
	push_bytes.resize(sizeof(GPUPushConstants));
	std::memcpy(push_bytes.ptrw(), &push, sizeof(GPUPushConstants));

	uint32_t groups = (pixel_count + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE;

	int64_t cl = rd_->compute_list_begin();
	rd_->compute_list_bind_compute_pipeline(cl, extend_pipeline_);
	rd_->compute_list_bind_uniform_set(cl, extend_uniform_set_, 0);
	rd_->compute_list_set_push_constant(cl, push_bytes, sizeof(GPUPushConstants));
	rd_->compute_list_dispatch(cl, groups, 1, 1);
	rd_->compute_list_end();
	rd_->submit();
}

void GPUPathTracer::_dispatch_shade(uint32_t pixel_count, uint32_t bounce) {
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");
	RT_ASSERT(pixel_count > 0, "pixel_count must be positive");
	RT_ASSERT(shade_uniform_set_.is_valid(), "Shade uniform set must be built");

	GPUPathTracePush push{};
	push.pixel_count = pixel_count;
	push.width       = cached_width_;
	push.height      = cached_height_;
	push.bounce      = bounce;
	push.max_bounces = cached_max_bounces_;
	push.sample_index = cached_sample_index_;
	push.light_count  = cached_light_count_;
	push.shadows_enabled = cached_shadows_enabled_ ? 1u : 0u;

	std::memcpy(push_data_cache_.ptrw(), &push, sizeof(GPUPathTracePush));

	uint32_t groups = (pixel_count + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE;

	int64_t cl = rd_->compute_list_begin();
	rd_->compute_list_bind_compute_pipeline(cl, shade_pipeline_);
	rd_->compute_list_bind_uniform_set(cl, shade_uniform_set_, 0);
	rd_->compute_list_set_push_constant(cl, push_data_cache_, sizeof(GPUPathTracePush));
	rd_->compute_list_dispatch(cl, groups, 1, 1);
	rd_->compute_list_end();
	rd_->submit();
}

void GPUPathTracer::_dispatch_connect(uint32_t pixel_count) {
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");
	RT_ASSERT(pixel_count > 0, "pixel_count must be positive");
	RT_ASSERT(connect_uniform_set_.is_valid(), "Connect uniform set must be built");

	// Stochastic single-light NEE: exactly one shadow ray per pixel.
	GPUPushConstants push{};
	push.ray_count  = pixel_count;
	push.query_mask = 0xFFFFFFFF;

	PackedByteArray push_bytes;
	push_bytes.resize(sizeof(GPUPushConstants));
	std::memcpy(push_bytes.ptrw(), &push, sizeof(GPUPushConstants));

	uint32_t groups = (pixel_count + WORKGROUP_SIZE - 1) / WORKGROUP_SIZE;

	int64_t cl = rd_->compute_list_begin();
	rd_->compute_list_bind_compute_pipeline(cl, connect_pipeline_);
	rd_->compute_list_bind_uniform_set(cl, connect_uniform_set_, 0);
	rd_->compute_list_set_push_constant(cl, push_bytes, sizeof(GPUPushConstants));
	rd_->compute_list_dispatch(cl, groups, 1, 1);
	rd_->compute_list_end();
	rd_->submit();
}

void GPUPathTracer::_readback_accumulation(float *color_output, uint32_t pixel_count) {
	RT_ASSERT_NOT_NULL(color_output);
	RT_ASSERT(rd_ != nullptr, "RenderingDevice must be valid");
	RT_ASSERT(pixel_count > 0, "pixel_count must be positive");

	// Ensure all GPU work is finished before readback.
	rd_->submit();
	rd_->sync();

	uint32_t byte_size = pixel_count * 4 * sizeof(float);
	PackedByteArray readback = rd_->buffer_get_data(accum_buffer_, 0, byte_size);

	RT_ASSERT(static_cast<uint32_t>(readback.size()) == byte_size,
		"Readback size mismatch — GPU buffer may be undersized");

	std::memcpy(color_output, readback.ptr(), byte_size);
}

// ============================================================================
// Resource cleanup
// ============================================================================

void GPUPathTracer::_free_wavefront_buffers() {
	RT_ASSERT(rd_ != nullptr || buffer_pixel_capacity_ == 0,
		"_free_wavefront_buffers: no RD but buffers allocated");
	RT_ASSERT(rd_ != nullptr || !ray_buffer_.is_valid(),
		"_free_wavefront_buffers: no RD but ray_buffer valid");

	if (!rd_) { return; }

	auto free_rid = [this](RID &rid) {
		if (rid.is_valid()) { rd_->free_rid(rid); rid = RID(); }
	};

	free_rid(ray_buffer_);
	free_rid(intersection_buffer_);
	free_rid(path_state_buffer_);
	free_rid(shadow_ray_buffer_);
	free_rid(shadow_result_buffer_);
	free_rid(accum_buffer_);

	// Invalidate descriptor sets (they reference freed buffers).
	free_rid(generate_uniform_set_);
	free_rid(extend_uniform_set_);
	free_rid(shade_uniform_set_);
	free_rid(connect_uniform_set_);

	buffer_pixel_capacity_ = 0;
}

void GPUPathTracer::_free_scene_buffers() {
	RT_ASSERT(rd_ != nullptr || uploaded_material_count_ == 0,
		"_free_scene_buffers: no RD but materials uploaded");
	RT_ASSERT(rd_ != nullptr || uploaded_triangle_count_ == 0,
		"_free_scene_buffers: no RD but triangles uploaded");

	if (!rd_) { return; }

	auto free_rid = [this](RID &rid) {
		if (rid.is_valid()) { rd_->free_rid(rid); rid = RID(); }
	};

	free_rid(material_buffer_);
	free_rid(material_id_buffer_);
	free_rid(light_buffer_);
	free_rid(env_buffer_);
	free_rid(camera_buffer_);
	free_rid(triangle_uv_buffer_);
	free_rid(triangle_normal_buffer_);
	free_rid(triangle_tangent_buffer_);
	free_rid(albedo_tex_array_);
	free_rid(normal_tex_array_);

	uploaded_material_count_ = 0;
	uploaded_triangle_count_ = 0;
}

void GPUPathTracer::_free_shaders() {
	RT_ASSERT(rd_ != nullptr || !generate_shader_.is_valid(),
		"_free_shaders: no RD but shaders exist");
	RT_ASSERT(rd_ != nullptr || !generate_pipeline_.is_valid(),
		"_free_shaders: no RD but pipelines exist");

	if (!rd_) { return; }

	auto free_rid = [this](RID &rid) {
		if (rid.is_valid()) { rd_->free_rid(rid); rid = RID(); }
	};

	free_rid(generate_pipeline_);
	free_rid(shade_pipeline_);
	free_rid(extend_pipeline_);
	free_rid(connect_pipeline_);

	free_rid(generate_shader_);
	free_rid(shade_shader_);
	free_rid(extend_shader_);
	free_rid(connect_shader_);
}
